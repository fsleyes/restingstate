---
title: "restingstate"
output: html_document
date: "2023-05-03"
---


```{r setup, include=FALSE}
library(tidyverse)


rawData <- read.csv(file = "restingstate_data.csv", header = TRUE)

data <- rawData %>% 
  filter(!is.na(pre_pair1)) %>%
  mutate(diff_pair1 = 0 - (post_pair1 - pre_pair1), #switching the signs
         diff_pair3 = post_pair3 - pre_pair3,
         diff_pair4 = post_pair4 - pre_pair4,
         diff_target = post.target - Pre.target,
         diff_control = post.control - pre.control)
data$pre_date <- lubridate::mdy(data$pre_date)
data$post_date <- lubridate::mdy(data$post_date)
data <- data %>% 
  mutate(diff_date = difftime(post_date, pre_date, units = "days")) %>%
  mutate(target_control_pre_raw = Pre.target - pre.control,
         target_control_post_raw = post.target - post.control,
         target_control_diff = target_control_post_raw - target_control_pre_raw)

```


correlations between the three pairs--it looks like pair 1 is best with r = .71
```{r}
cor_target_pair1 <- cor(data$diff_target, data$diff_pair1, method = "pearson")
cor_target_pair3 <- cor(data$diff_target, data$diff_pair3, method = "pearson")
cor_target_pair4 <- cor(data$diff_target, data$diff_pair4, method = "pearson")

cor_target_pair1
cor_target_pair3
cor_target_pair4
```

then just looking at pair 1, we can run a simple linear regression--we see that this is signficant with p = 0.0141
```{r}
reg_target_pair1 <- lm(diff_target ~ diff_pair1, data = data)
summary(reg_target_pair1)
```


controlling for date, phobias and dosage group
```{r}
reg_target_pair1_controlled.1 <- lm(diff_target ~ diff_pair1 + diff_date + num..phobias + dosage, data = data)
summary(reg_target_pair1_controlled.1)
```


michelle wanted to control for time between decoder/post, so here--is still significant with p = .0323
```{r}
reg_target_pair1_controlled.1 <- lm(diff_target ~ diff_pair1 + diff_date, data = data)
summary(reg_target_pair1_controlled.1)
```


now adding to the model by controlling for the control difference in activation--becomes only marginally significant with p = .0509

do we want to keep this model with these controls and try to do something else with it?
```{r}
reg_target_pair1_controlled.2 <- lm(diff_target ~ diff_pair1 + diff_date + diff_control, data = data)
summary(reg_target_pair1_controlled.2)
```


it becomes significant again if we control for number of phobias but do we have justification for this? may be promising?
```{r}
reg_target_pair1_controlled.3 <- lm(diff_target ~ diff_pair1 + diff_date + diff_control + num..phobias, data = data)
summary(reg_target_pair1_controlled.3)
```


i then went and subtracted the raw control scores from the raw pre scores and took those raw scores and subtracted pre from post and reran controlling for time, no significance
```{r}
reg_target_control_pair1 <- lm(target_control_diff ~ diff_pair1 + diff_date, data = data)
summary(reg_target_control_pair1)
```


now using induction average as outcome
```{r}
reg_induction_avg_pair1 <- lm(induction_avg ~ diff_pair1, data = data)
summary(reg_induction_avg_pair1)
```


also while controlling for date 
```{r}
reg_induction_avg_pair1_controlled.1 <- lm(induction_avg ~ diff_pair1 + diff_date, data = data)
summary(reg_induction_avg_pair1_controlled.1)
```

no significance with either models for induction average--what about induction learning (post - pre)?
now using induction average as outcome
```{r}
reg_induction_learning_pair1 <- lm(induction_learning ~ diff_pair1, data = data)
summary(reg_induction_learning_pair1)
```


also while controlling for date--big difference? wondering why, also weird how time is significant as well
```{r}
reg_induction_learning_pair1_controlled.1 <- lm(induction_learning ~ diff_pair1 + diff_date, data = data)
summary(reg_induction_learning_pair1_controlled.1)
```

induction learning with date, phobias, dosage
```{r}
reg_induction_learning_pair1_controlled.1 <- lm(induction_learning ~ diff_pair1 + diff_date + num..phobias + dosage, data = data)
summary(reg_induction_learning_pair1_controlled.1)
```




scatterplot for induction learning
```{r}
ind_learn_scatter <- ggplot(data, aes(x = diff_pair1, y = induction_learning)) +
  geom_point() +
  geom_smooth(method = lm, se = FALSE) +
  labs(x = "Change in Resting State Network",
       y = "Induction Learning Score") +
  theme_classic() +
  theme(axis.title = element_text(size = 15),
        axis.text = element_text(size = 11, colour = "black"),
        axis.title.y = element_text(margin = margin(t = 0, r = 15, b = 0, l = 0)),
        axis.title.x = element_text(margin = margin(t = 15)))
ind_learn_scatter

ggsave("ind_learn_scatter.png", width = 6, height = 4, dpi = 600, units = "in")
```


scatterplot with the amygdala changes
```{r}
amyg_scatter <- ggplot(data, aes(x = diff_pair1, y = diff_target)) +
  geom_point() +
  stat_smooth(method = lm, se = FALSE, fullrange = TRUE)+
  labs(x = "Change in Resting State Network",
       y = "Change in Amygdala Activation") +
  theme_classic() +
  theme(axis.title = element_text(size = 15),
        axis.text = element_text(size = 11, colour = "black"),
        axis.title.y = element_text(margin = margin(t = 0, r = 15, b = 0, l = 0)),
        axis.title.x = element_text(margin = margin(t = 15))) 
  
amyg_scatter

ggsave("amyg_scatter.png", dpi = 600, width = 6, height = 4, units = "in")
```


looks like the MORE change in RSN, the WORSE induction learning is and the MORE changes in amygdala--these two findings are a bit in conflict with one another

how does the amygdala reactivity relate to the induction learning?
```{r}
amyg_induct <- ggplot(data, aes(x = diff_target, y = induction_learning)) +
  geom_point() +
  geom_smooth(method = lm, se = FALSE)

amyg_induct
```
it looks like the greater your change in amygdala, the better you are at induction learning--however, the direction that these are in means that the those who experienced increases in amygdala (because post - pre) had better induction learning??


what happens if we remove that one participant who has the low diff_pair1 score?
it loses significance in the simple regression model for amygdala
and becomes marginally significant in the regression model for induction learning

```{r}
removedData <- data[-c(10),]

reg_target_pair1 <- lm(diff_target ~ diff_pair1, data = removedData)
summary(reg_target_pair1)

reg_induction_learning_pair1_controlled.1 <- lm(induction_learning ~ diff_pair1 + diff_date + num..phobias + dosage, data = removedData)
summary(reg_induction_learning_pair1_controlled.1)


test <- ggplot(removedData, aes(x = diff_pair1, y = induction_learning)) +
  geom_point()
test
```
